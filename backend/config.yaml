# MacroState Control Room — Extended Configuration
# Fed Policy + Inflation-Aware PPO

# ── Policy engine mode ──────────────────────────────────────────
# "heuristic" = existing Monte Carlo optimizer
# "rl"        = PPO-trained reinforcement learning agent
policy_mode: "heuristic"

# ── Data architecture ───────────────────────────────────────────
data:
  start_year: 1990
  frequency: "monthly"          # training frequency for RL
  use_daily_cache: true         # store daily raw data before aggregation
  inflation_target: 0.02        # 2 % annual CPI target

# ── Transition model ────────────────────────────────────────────
transition:
  model_type: "linear"          # "linear" | "neural"
  inflation_persistence: 0.95   # AR(1) coefficient for inflation gap
  neural:
    hidden_dim: 64
    n_layers: 2
    use_layer_norm: true
    learning_rate: 1.0e-3
    weight_decay: 1.0e-4
    epochs: 100
    early_stopping_patience: 10
    train_split: 0.8
    seed: 42
    checkpoint_path: "models/checkpoints/neural_transition.pt"

# ── Gym environment ─────────────────────────────────────────────
gym_env:
  episode_length: 60            # months
  max_rate_step: 0.5            # max Δ federal funds rate per step (% points)
  zero_lower_bound: true        # enforce r_t ≥ 0
  state_dim: 12                 # 4 latent + 3 regime + 1 last_action + 1 fed_rate + 3 eigen

# ── PPO training ────────────────────────────────────────────────
ppo:
  total_timesteps: 500000
  n_steps: 2048
  batch_size: 64
  gamma: 0.99
  gae_lambda: 0.95
  learning_rate: 3.0e-4
  clip_range: 0.2
  ent_coef: 0.01
  n_epochs: 10
  policy_kwargs:
    net_arch: [64, 64]
    activation_fn: "Tanh"
  seed: 42
  checkpoint_path: "models/checkpoints/ppo_policy.zip"
  tensorboard_log: "runs/ppo_macro"

# ── Reward function ─────────────────────────────────────────────
reward:
  w_stress: 1.0
  w_inflation: 1.0
  w_crisis: 2.0
  w_rate_change: 0.1
  w_taylor: 0.0                 # optional future extension
  inflation_target: 0.02
